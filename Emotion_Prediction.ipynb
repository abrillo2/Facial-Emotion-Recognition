{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The Face data we used for training is taken from kaggle in Representation Learning: Facial Expression Recognition Challenge from 2013. the data consists of 48x48 pixel grayscale images of faces arranged in a row and seven categories/labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) of emotions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "#compie and fit the CNN model\n",
    "def comFit(model,trainX, trainY, testX=None, testY=None):\n",
    "    #Compliling the model\n",
    "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    #Training the model\n",
    "    model.fit(trainX, trainY,batch_size=64,epochs=32,verbose=1,shuffle=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,emotions):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "\n",
    "        if not val:\n",
    "            continue;\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.32, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=8)\n",
    "\n",
    "            predImg=testImg[y:y+w,x:x+h]\n",
    "            predImg=cv2.resize(predImg,(48,48))\n",
    "            predPixels = image.img_to_array(predImg)\n",
    "            predPixels = np.expand_dims(predPixels, axis = 0)\n",
    "            predPixels /= 255\n",
    "\n",
    "            pridectedList = model.predict(predPixels)\n",
    "\n",
    "            maxVal = np.argmax(pridectedList[0])\n",
    "            predEmotion = emotions[maxVal]\n",
    "\n",
    "            cv2.putText(img, predEmotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(img, (1000, 700))\n",
    "        cv2.imshow('Analayzing Facial Emotion',resized_img)\n",
    "\n",
    "        if cv2.waitKey(10) == ord('e'):\n",
    "            break\n",
    "\n",
    "    faceDetectioin.release()\n",
    "    cv2.destroyAllWindows\n",
    "#CNN Model from training set\n",
    "def craeateModel(trainX):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(trainX.shape[1:])))\n",
    "    model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible target emotions\n",
    "emotions = ('happy', 'fear', 'disgust', 'angry', 'sad', 'surprise', 'neutral')\n",
    "#load the face data using panda from csv and extract triaining data from it.\n",
    "faceData = pd.read_csv('fer2013.csv').iterrows()\n",
    "\n",
    "trainingSize = 5000\n",
    "trainY = []\n",
    "trainX = []\n",
    "\n",
    "for i,j in faceData:\n",
    "    \n",
    "    if(trainingSize == 0):\n",
    "        break;\n",
    "    else:\n",
    "        trainX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "        trainY.append(j['emotion'])\n",
    "    \n",
    "    trainingSize-=1\n",
    "\n",
    "#flatten the data array and normalize between 1 and 0 using mean and standard deviation for x\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "trainY = np_utils.to_categorical(trainY, num_classes=7)\n",
    "\n",
    "trainX -= np.mean(trainX, axis=0)\n",
    "trainX /= np.std(trainX, axis=0)\n",
    "#reshape the matrices to represent each face data\n",
    "trainX = trainX.reshape(trainX.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature filtering\n",
    "We used a three layer Convolutional neural network for extracting features and we aplied maxpooling to reduce the dimensions of the data. Finally we flatten the image matrix and pass it through a fully connected layer to classify the images. \n",
    "\n",
    "since we only have one input(image pixels) and one output(the pridicted emotion, 0 to 6 ),We used sequential class from keras for creating the CNN model. We add each layer instance with conv2d method, which  creates convolution kernel(4,4) that is convolved with the layer input to produce a tensor of outputs.The activatioin function used is relu, which activiate each neuron(not all at once) to produce output for the next layer. The drop out rate used is 0.2.\n",
    "\n",
    "MaxPolling2D is used with window size and stride of 3 in each dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = craeateModel(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compile and train the model\n",
    "\n",
    "in the model configuration, with use adam optimizer and 'accuracy' as a metric to evaluate the model during the training and testing. And a loss/objective function used is crossentropy.\n",
    "\n",
    "in the triaining of the model, number of samples per gradient update is set to be 64, max iteration is set to be 32, verbosity mode used is progress bar during training and shuffle is on , to shuffle the training data before each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "79/79 [==============================] - 63s 797ms/step - loss: 1.8397 - accuracy: 0.2161\n",
      "Epoch 2/32\n",
      "79/79 [==============================] - 66s 833ms/step - loss: 1.7861 - accuracy: 0.2585\n",
      "Epoch 3/32\n",
      "79/79 [==============================] - 65s 826ms/step - loss: 1.7555 - accuracy: 0.2756\n",
      "Epoch 4/32\n",
      "79/79 [==============================] - 73s 922ms/step - loss: 1.6700 - accuracy: 0.3379\n",
      "Epoch 5/32\n",
      "79/79 [==============================] - 69s 877ms/step - loss: 1.6091 - accuracy: 0.3604\n",
      "Epoch 6/32\n",
      "79/79 [==============================] - 71s 904ms/step - loss: 1.5650 - accuracy: 0.3716\n",
      "Epoch 7/32\n",
      "79/79 [==============================] - 69s 872ms/step - loss: 1.4949 - accuracy: 0.4151\n",
      "Epoch 8/32\n",
      "79/79 [==============================] - 67s 845ms/step - loss: 1.5000 - accuracy: 0.4095\n",
      "Epoch 9/32\n",
      "79/79 [==============================] - 70s 883ms/step - loss: 1.4422 - accuracy: 0.4342\n",
      "Epoch 10/32\n",
      "79/79 [==============================] - 66s 844ms/step - loss: 1.4043 - accuracy: 0.4488\n",
      "Epoch 11/32\n",
      "79/79 [==============================] - 68s 867ms/step - loss: 1.3646 - accuracy: 0.4720\n",
      "Epoch 12/32\n",
      "79/79 [==============================] - 72s 913ms/step - loss: 1.3242 - accuracy: 0.4971\n",
      "Epoch 13/32\n",
      "79/79 [==============================] - 72s 906ms/step - loss: 1.3107 - accuracy: 0.4941\n",
      "Epoch 14/32\n",
      "79/79 [==============================] - 71s 899ms/step - loss: 1.2655 - accuracy: 0.5270\n",
      "Epoch 15/32\n",
      "79/79 [==============================] - 71s 900ms/step - loss: 1.2375 - accuracy: 0.5298\n",
      "Epoch 16/32\n",
      "79/79 [==============================] - 72s 901ms/step - loss: 1.1947 - accuracy: 0.5388\n",
      "Epoch 17/32\n",
      "79/79 [==============================] - 73s 932ms/step - loss: 1.1497 - accuracy: 0.5526\n",
      "Epoch 18/32\n",
      "79/79 [==============================] - 75s 949ms/step - loss: 1.1099 - accuracy: 0.5662\n",
      "Epoch 19/32\n",
      "79/79 [==============================] - 75s 947ms/step - loss: 1.0698 - accuracy: 0.5776\n",
      "Epoch 20/32\n",
      "79/79 [==============================] - 72s 911ms/step - loss: 1.0040 - accuracy: 0.6190\n",
      "Epoch 21/32\n",
      "79/79 [==============================] - 67s 850ms/step - loss: 0.9888 - accuracy: 0.6153\n",
      "Epoch 22/32\n",
      "79/79 [==============================] - 68s 862ms/step - loss: 0.9223 - accuracy: 0.6463\n",
      "Epoch 23/32\n",
      "75/79 [===========================>..] - ETA: 3s - loss: 0.8727 - accuracy: 0.6585"
     ]
    }
   ],
   "source": [
    "model =  comFit(model,trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Facial Emotion Prediction\n",
    "\n",
    "capturing live frames:\n",
    "we used pc camera as source of video and we used openCV library.we used VideoCapture class with '0' as an argument for capturing videos from the default source of video(camera). We created an infinite loop to capture frames  from the vieo iteratively using the read method of VideoCapture class and convert the frame into grayscale.\n",
    "\n",
    "detecting facial region:\n",
    "We again make use of openCV's face detection api CascadeClassifier, to detect facial arias of the captured frame. The training data set we use for CascadeClassifier is haaarcascade_frontalface_default.xml. detectMultiScale returns the detected faces as list of rectangles and their locations.\n",
    "\n",
    "prediction:\n",
    "We loop through the detected face recangles and draw the rectangles on the screen. we select area of interst from the image based on the returned rectangle and the location of the image. convert the selected area into array of metrices containing pixles after resizing the image based on our training data. reduce the rgb representation by dividing it by 255 and give the image to our model for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'cv2.CascadeClassifier' object has no attribute 'release'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4dd572876a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mfaceDetectioin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'cv2.CascadeClassifier' object has no attribute 'release'"
     ]
    }
   ],
   "source": [
    "realPred(0,model,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
