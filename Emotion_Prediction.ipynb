{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The Face data we used for training is taken from kaggle in Representation Learning: Facial Expression Recognition Challenge from 2013. the data consists of 48x48 pixel grayscale images of faces arranged in a row and seven categories/labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) of emotions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def xyFromDataset(xT,yT):\n",
    "    \n",
    "    #directories of images for each 7 emotions\n",
    "    category = [cat for cat in os.listdir('training_sett')]\n",
    "    \n",
    "    #iterate through each category to extract image data set for each emotion\n",
    "    for catT in category:\n",
    "        current_image_path = os.path.join('training_sett', catT)\n",
    "        imageList = [i for i in os.listdir(current_image_path) if not i.startswith('.')]\n",
    "        \n",
    "        #read image data as piexels and prepare training variables x,y \n",
    "        for i in imageList:\n",
    "            imageT = cv2.imread(os.path.join(current_image_path, i))                \n",
    "            imageT = cv2.cvtColor(imageT, cv2.COLOR_BGR2GRAY)\n",
    "            tI=cv2.resize(imageT,(48,48))\n",
    "            trainPixels = image.img_to_array(tI)\n",
    "            trainPixels = np.expand_dims(trainPixels, axis = 0)\n",
    "            trainPixels /= 255  \n",
    "            xT.append(np.array(trainPixels,\"float32\"))\n",
    "            yT.append(float(catT))\n",
    "    #finalizing\n",
    "    xT = np.array(xT)\n",
    "    yT = np.array(yT,'float32')\n",
    "    yT = np_utils.to_categorical(yT, num_classes=7)\n",
    "\n",
    "    xT -= np.mean(xT, axis=0)\n",
    "    xT /= np.std(xT, axis=0)\n",
    "    \n",
    "    xT = xT.reshape(xT.shape[0], 48, 48, 1)\n",
    "    \n",
    "    offset = int(xT.shape[0]-xT.shape[0]/4)\n",
    "\n",
    "    xTT = xT[0:offset]\n",
    "    yTT = yT[0:offset]\n",
    "    xV = xT[offset:]\n",
    "    yV = yT[offset:]\n",
    "    \n",
    "    return xTT, yTT,xV,yV\n",
    "\n",
    "#capture training data\n",
    "def gatherData(source,emotions):\n",
    "    \n",
    "    #local variables for file name and directory name\n",
    "    i=0\n",
    "    label = 0\n",
    "    m = \"/record\"\n",
    "    a = str(time.time())\n",
    "    \n",
    "    #current emotion data save location\n",
    "    categoryPath = os.path.join('training_sett', str(label))\n",
    "    \n",
    "    #start capturing data using opencv and load face detection dataset 'frontalface_default.xml'\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    #track key press\n",
    "    K = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        #capture frame from source\n",
    "        val, img = liveFrame.read()\n",
    "        \n",
    "        #if no frame is captured continue\n",
    "        if not val:\n",
    "            continue;\n",
    "        \n",
    "        #convert captured image into grayscale for management ease\n",
    "        imageT = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #invoke face detection on the captured frame\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(imageT, 1.32, 5)\n",
    "        \n",
    "        #loop through detected face list and save each face to the corresponding emotion directory label\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "            cv2.putText(img, emotions[label] + m, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "            \n",
    "            if K == ord('w') or m == \"/recording\":    \n",
    "                \n",
    "                m = \"/recording\"\n",
    "                cp = os.path.join(categoryPath, a + str(i) + \".jpg\")\n",
    "                predImg=imageT[y:y+w,x:x+h]\n",
    "                predImg=cv2.resize(predImg,(48,48))\n",
    "                cv2.imwrite(cp, predImg)\n",
    "                i+=1\n",
    "                print (cp)\n",
    "                \n",
    "            if K == ord('q') and m == \"/recording\":\n",
    "                \n",
    "                m = \"/record\"\n",
    "                if(label < 6):\n",
    "                    label+=1\n",
    "                    categoryPath = os.path.join('training_sett', str(label))\n",
    "                else:\n",
    "                    break \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        #open openCV's display window with specified frame size\n",
    "        resized_img = cv2.resize(img, (1000, 700))\n",
    "        cv2.imshow('Collecting Data',resized_img)\n",
    "        K = cv2.waitKey(10) & 0xFF\n",
    "        \n",
    "        #close openCV's window if key 'e' is pressed\n",
    "        if K == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "    \n",
    "    \n",
    "#user fer 2013 instead\n",
    "def xyFromDatasetFer2013():\n",
    "    #prepare training variables using fer2013 dataset\n",
    "    #load the face data using panda from csv and extract triaining data from it.\n",
    "    faceData = pd.read_csv('fer2013.csv').iterrows()\n",
    "\n",
    "    trainY = []\n",
    "    trainX = []\n",
    "\n",
    "    textY = []\n",
    "    testX = []\n",
    "\n",
    "    for i,j in faceData:\n",
    "\n",
    "        if 'Training' in j['Usage']:\n",
    "            trainX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "            trainY.append(j['emotion'])\n",
    "\n",
    "        elif 'PublicTest' in j['Usage']:\n",
    "            testX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "            textY.append(j['emotion'])\n",
    "    #flatten the data array and normalize between 1 and 0 using mean and standard deviation for x\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    trainY = np_utils.to_categorical(trainY, num_classes=7)\n",
    "\n",
    "    trainX -= np.mean(trainX, axis=0)\n",
    "    trainX /= np.std(trainX, axis=0)\n",
    "\n",
    "    testX = np.array(testX)\n",
    "    textY = np.array(textY)\n",
    "    textY = np_utils.to_categorical(textY, num_classes=7)\n",
    "\n",
    "    testX -= np.mean(testX, axis=0)\n",
    "    testX /= np.std(testX, axis=0)\n",
    "\n",
    "\n",
    "    #reshape the matrices to represent each face data\n",
    "    trainX = trainX.reshape(trainX.shape[0], 48, 48, 1)\n",
    "    testX = testX.reshape(testX.shape[0], 48, 48, 1)\n",
    "    \n",
    "    return trainX,trainY,testX,textY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compie and fit the CNN model\n",
    "def comFit(model,trainX, trainY,epoch,bs,testX=None, testY=None):\n",
    "    #Compliling the model\n",
    "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    #Training the model\n",
    "    model.fit(trainX, trainY,epochs=epoch, batch_size=bs,verbose=1,shuffle=True, validation_data=(testX, testY),)\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,emotions):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "\n",
    "        if not val:\n",
    "            continue;\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.32, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "\n",
    "            predImg=testImg[y:y+w,x:x+h]\n",
    "            predImg=cv2.resize(predImg,(48,48))\n",
    "            predPixels = image.img_to_array(predImg)\n",
    "            predPixels = np.expand_dims(predPixels, axis = 0)\n",
    "            predPixels /= 255\n",
    "\n",
    "            pridectedList = model.predict(predPixels)\n",
    "           \n",
    "            maxVal = np.argmax(pridectedList[0])\n",
    "            predEmotion = emotions[maxVal]\n",
    "            \n",
    "            path = os.path.join(\"emot\",predEmotion+\".png\")\n",
    "          \n",
    "            \n",
    "            emoji = cv2.imread(path,-1)\n",
    "            emoji = cv2.resize(emoji,(50,50))\n",
    "            \n",
    "            y1, y2 = int(y), int(y) + emoji.shape[0]\n",
    "            x1, x2 = int(x), int(x) + emoji.shape[1]\n",
    "            \n",
    "            ealpha = emoji[:, :, 3] / 255.0\n",
    "            ialpha = 1.0 - ealpha\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                img[y1:y2, x1:x2, c] = (ealpha * emoji[:, :, c] + ialpha * img[y1:y2, x1:x2, c])\n",
    "            \n",
    "            cv2.putText(img, predEmotion, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(img, (1000, 600))\n",
    "        cv2.imshow('Analayzing Facial Emotion',resized_img)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "    #exit()\n",
    "#CNN Model from training set\n",
    "def craeateModel(trainX,labels):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(trainX.shape[1:])))\n",
    "    model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    '''    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))'''\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(labels, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible target emotions\n",
    "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "\n",
    "#training and testing data xy\n",
    "xT,yT,xV,yV = [],[],[],[]\n",
    "\n",
    "val = xyFromDataset(xT,yT)\n",
    "\n",
    "xT = np.asarray(val[0])\n",
    "yT = np.asarray(val[1])\n",
    "xV = np.asarray(val[2])\n",
    "yV = np.asarray(val[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature filtering\n",
    "We used a three layer Convolutional neural network for extracting features and we aplied maxpooling to reduce the dimensions of the data. Finally we flatten the image matrix and pass it through a fully connected layer to classify the images. \n",
    "\n",
    "since we only have one input(image pixels) and one output(the pridicted emotion, 0 to 6 ),We used sequential class from keras for creating the CNN model. We add each layer instance with conv2d method, which  creates convolution kernel(4,4) that is convolved with the layer input to produce a tensor of outputs.The activatioin function used is relu, which activiate each neuron(not all at once) to produce output for the next layer. The drop out rate used is 0.2.\n",
    "\n",
    "MaxPolling2D is used with window size and stride of 3 in each dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = craeateModel(xT,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compile and train the model\n",
    "\n",
    "in the model configuration, with use adam optimizer and 'accuracy' as a metric to evaluate the model during the training and testing. And a loss/objective function used is crossentropy.\n",
    "\n",
    "in the triaining of the model, number of samples per gradient update is set to be 64, max iteration is set to be 32, verbosity mode used is progress bar during training and shuffle is on , to shuffle the training data before each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 35s 442ms/step - loss: 1.3933 - accuracy: 0.5860 - val_loss: 7.5493 - val_accuracy: 0.2919\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 37s 465ms/step - loss: 0.0962 - accuracy: 0.9683 - val_loss: 8.3116 - val_accuracy: 0.3493\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 37s 470ms/step - loss: 0.0318 - accuracy: 0.9909 - val_loss: 7.9537 - val_accuracy: 0.3385\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 37s 471ms/step - loss: 0.0253 - accuracy: 0.9914 - val_loss: 5.1796 - val_accuracy: 0.3624\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 39s 490ms/step - loss: 0.0248 - accuracy: 0.9930 - val_loss: 8.1491 - val_accuracy: 0.3541\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 38s 485ms/step - loss: 0.0255 - accuracy: 0.9944 - val_loss: 7.3745 - val_accuracy: 0.3385\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 37s 473ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 10.1776 - val_accuracy: 0.3577\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 37s 473ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 7.9012 - val_accuracy: 0.3589\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 39s 499ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 10.5360 - val_accuracy: 0.3600\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 38s 484ms/step - loss: 0.0049 - accuracy: 0.9978 - val_loss: 9.5049 - val_accuracy: 0.3553\n"
     ]
    }
   ],
   "source": [
    "model =  comFit(model,xT, yT,10,32,xV,yV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Facial Emotion Prediction\n",
    "\n",
    "capturing live frames:\n",
    "we used pc camera as source of video and we used openCV library.we used VideoCapture class with '0' as an argument for capturing videos from the default source of video(camera). We created an infinite loop to capture frames  from the vieo iteratively using the read method of VideoCapture class and convert the frame into grayscale.\n",
    "\n",
    "detecting facial region:\n",
    "We again make use of openCV's face detection api CascadeClassifier, to detect facial arias of the captured frame. The training data set we use for CascadeClassifier is haaarcascade_frontalface_default.xml. detectMultiScale returns the detected faces as list of rectangles and their locations.\n",
    "\n",
    "prediction:\n",
    "We loop through the detected face recangles and draw the rectangles on the screen. we select area of interst from the image based on the returned rectangle and the location of the image. convert the selected area into array of metrices containing pixles after resizing the image based on our training data. reduce the rgb representation by dividing it by 255 and give the image to our model for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our model for later prediction use\n",
    "modelJson = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(modelJson)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our model for prediction\n",
    "'''model = model_from_json(open(\"model3.json\", \"r\").read())\n",
    "model.load_weights('model3.h5')\n",
    "'''\n",
    "realPred(0,model,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.50487995147705\n",
      "Test accuracy: 0.3552631437778473\n"
     ]
    }
   ],
   "source": [
    "# evaluation of our model\n",
    "score = model.evaluate(xV, yV, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
