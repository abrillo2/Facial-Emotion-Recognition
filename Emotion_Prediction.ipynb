{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The Face data we used for training is taken from kaggle in Representation Learning: Facial Expression Recognition Challenge from 2013. the data consists of 48x48 pixel grayscale images of faces arranged in a row and seven categories/labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) of emotions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "#compie and fit the CNN model\n",
    "def comFit(model,trainX, trainY, testX=None, testY=None):\n",
    "    #Compliling the model\n",
    "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    #Training the model\n",
    "    model.fit(trainX, trainY,batch_size=64,epochs=32,verbose=1,shuffle=True, validation_data=(testX, testY),)\n",
    "    \n",
    "    #model = model_from_json(open(\"fer.json\", \"r\").read())\n",
    "    #model.load_weights('fer.h5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,emotions):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "\n",
    "        if not val:\n",
    "            continue;\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.3, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "\n",
    "            predImg=testImg[y:y+w,x:x+h]\n",
    "            predImg=cv2.resize(predImg,(48,48))\n",
    "            predPixels = image.img_to_array(predImg)\n",
    "            predPixels = np.expand_dims(predPixels, axis = 0)\n",
    "            predPixels /= 255\n",
    "\n",
    "            pridectedList = model.predict(predPixels)\n",
    "\n",
    "            maxVal = np.argmax(pridectedList[0])\n",
    "            predEmotion = emotions[maxVal]\n",
    "            \n",
    "            path = os.path.join(\"emot\",predEmotion+\".png\")\n",
    "          \n",
    "            \n",
    "            emoji = cv2.imread(path,-1)\n",
    "            emoji = cv2.resize(emoji,(50,50))\n",
    "            \n",
    "            y1, y2 = int(y), int(y) + emoji.shape[0]\n",
    "            x1, x2 = int(x), int(x) + emoji.shape[1]\n",
    "            \n",
    "            ealpha = emoji[:, :, 3] / 255.0\n",
    "            ialpha = 1.0 - ealpha\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                img[y1:y2, x1:x2, c] = (ealpha * emoji[:, :, c] + ialpha * img[y1:y2, x1:x2, c])\n",
    "            \n",
    "            cv2.putText(img, predEmotion, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Analayzing Facial Emotion',img)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "    #exit()\n",
    "#CNN Model from training set\n",
    "def craeateModel(trainX):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(trainX.shape[1:])))\n",
    "    model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible target emotions\n",
    "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "#load the face data using panda from csv and extract triaining data from it.\n",
    "faceData = pd.read_csv('fer2013.csv').iterrows()\n",
    "\n",
    "trainingSize = 5000\n",
    "testingSize =  2000\n",
    "\n",
    "trainY = []\n",
    "trainX = []\n",
    "\n",
    "textY = []\n",
    "testX = []\n",
    "\n",
    "for i,j in faceData:\n",
    "    \n",
    "    if(trainingSize == 0):\n",
    "        break;\n",
    "    else:\n",
    "        trainX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "        trainY.append(j['emotion'])\n",
    "    \n",
    "    trainingSize-=1\n",
    "    \n",
    "for i,j in faceData:\n",
    "      \n",
    "    if testingSize == 0:\n",
    "        break\n",
    "    \n",
    "    if 'PublicTest' in j['Usage']:\n",
    "        testX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "        textY.append(j['emotion'])\n",
    "        \n",
    "        testingSize -=1\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#flatten the data array and normalize between 1 and 0 using mean and standard deviation for x\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "trainY = np_utils.to_categorical(trainY, num_classes=7)\n",
    "\n",
    "trainX -= np.mean(trainX, axis=0)\n",
    "trainX /= np.std(trainX, axis=0)\n",
    "\n",
    "testX = np.array(testX)\n",
    "textY = np.array(textY)\n",
    "textY = np_utils.to_categorical(textY, num_classes=7)\n",
    "\n",
    "testX -= np.mean(testX, axis=0)\n",
    "testX /= np.std(testX, axis=0)\n",
    "#reshape the matrices to represent each face data\n",
    "trainX = trainX.reshape(trainX.shape[0], 48, 48, 1)\n",
    "testX = testX.reshape(testX.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature filtering\n",
    "We used a three layer Convolutional neural network for extracting features and we aplied maxpooling to reduce the dimensions of the data. Finally we flatten the image matrix and pass it through a fully connected layer to classify the images. \n",
    "\n",
    "since we only have one input(image pixels) and one output(the pridicted emotion, 0 to 6 ),We used sequential class from keras for creating the CNN model. We add each layer instance with conv2d method, which  creates convolution kernel(4,4) that is convolved with the layer input to produce a tensor of outputs.The activatioin function used is relu, which activiate each neuron(not all at once) to produce output for the next layer. The drop out rate used is 0.2.\n",
    "\n",
    "MaxPolling2D is used with window size and stride of 3 in each dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = craeateModel(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compile and train the model\n",
    "\n",
    "in the model configuration, with use adam optimizer and 'accuracy' as a metric to evaluate the model during the training and testing. And a loss/objective function used is crossentropy.\n",
    "\n",
    "in the triaining of the model, number of samples per gradient update is set to be 64, max iteration is set to be 32, verbosity mode used is progress bar during training and shuffle is on , to shuffle the training data before each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "79/79 [==============================] - 68s 856ms/step - loss: 1.8397 - accuracy: 0.2461 - val_loss: 1.8059 - val_accuracy: 0.2380\n",
      "Epoch 2/32\n",
      "79/79 [==============================] - 68s 863ms/step - loss: 1.7924 - accuracy: 0.2492 - val_loss: 1.7688 - val_accuracy: 0.2560\n",
      "Epoch 3/32\n",
      "79/79 [==============================] - 69s 871ms/step - loss: 1.7454 - accuracy: 0.2972 - val_loss: 1.6785 - val_accuracy: 0.3090\n",
      "Epoch 4/32\n",
      "79/79 [==============================] - 68s 866ms/step - loss: 1.6983 - accuracy: 0.3167 - val_loss: 1.6608 - val_accuracy: 0.3225\n",
      "Epoch 5/32\n",
      "79/79 [==============================] - 69s 876ms/step - loss: 1.6345 - accuracy: 0.3578 - val_loss: 1.5781 - val_accuracy: 0.3760\n",
      "Epoch 6/32\n",
      "79/79 [==============================] - 70s 893ms/step - loss: 1.5663 - accuracy: 0.3743 - val_loss: 1.5794 - val_accuracy: 0.3680\n",
      "Epoch 7/32\n",
      "79/79 [==============================] - 71s 901ms/step - loss: 1.5217 - accuracy: 0.4072 - val_loss: 1.5509 - val_accuracy: 0.3925\n",
      "Epoch 8/32\n",
      "79/79 [==============================] - 70s 884ms/step - loss: 1.5154 - accuracy: 0.4046 - val_loss: 1.5030 - val_accuracy: 0.3895\n",
      "Epoch 9/32\n",
      "79/79 [==============================] - 68s 869ms/step - loss: 1.4564 - accuracy: 0.4407 - val_loss: 1.4939 - val_accuracy: 0.4200\n",
      "Epoch 10/32\n",
      "79/79 [==============================] - 71s 900ms/step - loss: 1.4281 - accuracy: 0.4499 - val_loss: 1.4557 - val_accuracy: 0.4150\n",
      "Epoch 11/32\n",
      "79/79 [==============================] - 71s 886ms/step - loss: 1.3688 - accuracy: 0.4632 - val_loss: 1.4593 - val_accuracy: 0.4475\n",
      "Epoch 12/32\n",
      "79/79 [==============================] - 71s 896ms/step - loss: 1.3692 - accuracy: 0.4656 - val_loss: 1.4974 - val_accuracy: 0.4280\n",
      "Epoch 13/32\n",
      "79/79 [==============================] - 71s 898ms/step - loss: 1.2953 - accuracy: 0.4958 - val_loss: 1.4684 - val_accuracy: 0.4415\n",
      "Epoch 14/32\n",
      "79/79 [==============================] - 71s 900ms/step - loss: 1.2808 - accuracy: 0.5104 - val_loss: 1.4572 - val_accuracy: 0.4485\n",
      "Epoch 15/32\n",
      "79/79 [==============================] - 71s 905ms/step - loss: 1.2274 - accuracy: 0.5354 - val_loss: 1.4228 - val_accuracy: 0.4510\n",
      "Epoch 16/32\n",
      "79/79 [==============================] - 70s 895ms/step - loss: 1.1857 - accuracy: 0.5472 - val_loss: 1.4169 - val_accuracy: 0.4520\n",
      "Epoch 17/32\n",
      "79/79 [==============================] - 72s 912ms/step - loss: 1.1715 - accuracy: 0.5438 - val_loss: 1.4527 - val_accuracy: 0.4570\n",
      "Epoch 18/32\n",
      "79/79 [==============================] - 77s 974ms/step - loss: 1.1316 - accuracy: 0.5598 - val_loss: 1.5262 - val_accuracy: 0.4600\n",
      "Epoch 19/32\n",
      "79/79 [==============================] - 83s 1s/step - loss: 1.1060 - accuracy: 0.5726 - val_loss: 1.5372 - val_accuracy: 0.4535\n",
      "Epoch 20/32\n",
      "79/79 [==============================] - 87s 1s/step - loss: 1.0466 - accuracy: 0.5926 - val_loss: 1.5261 - val_accuracy: 0.4510\n",
      "Epoch 21/32\n",
      "79/79 [==============================] - 72s 909ms/step - loss: 0.9803 - accuracy: 0.6150 - val_loss: 1.5518 - val_accuracy: 0.4560\n",
      "Epoch 22/32\n",
      "79/79 [==============================] - 74s 946ms/step - loss: 0.9426 - accuracy: 0.6314 - val_loss: 1.5783 - val_accuracy: 0.4555\n",
      "Epoch 23/32\n",
      "79/79 [==============================] - 72s 914ms/step - loss: 0.8907 - accuracy: 0.6642 - val_loss: 1.6361 - val_accuracy: 0.4650\n",
      "Epoch 24/32\n",
      "79/79 [==============================] - 70s 894ms/step - loss: 0.8680 - accuracy: 0.6741 - val_loss: 1.6494 - val_accuracy: 0.4615\n",
      "Epoch 25/32\n",
      "79/79 [==============================] - 73s 927ms/step - loss: 0.8327 - accuracy: 0.6797 - val_loss: 1.7607 - val_accuracy: 0.4550\n",
      "Epoch 26/32\n",
      "79/79 [==============================] - 73s 930ms/step - loss: 0.7817 - accuracy: 0.6989 - val_loss: 1.8334 - val_accuracy: 0.4435\n",
      "Epoch 27/32\n",
      "79/79 [==============================] - 72s 912ms/step - loss: 0.7237 - accuracy: 0.7267 - val_loss: 1.7867 - val_accuracy: 0.4510\n",
      "Epoch 28/32\n",
      "79/79 [==============================] - 69s 878ms/step - loss: 0.6906 - accuracy: 0.7347 - val_loss: 1.9184 - val_accuracy: 0.4525\n",
      "Epoch 29/32\n",
      "79/79 [==============================] - 72s 914ms/step - loss: 0.6902 - accuracy: 0.7445 - val_loss: 1.8697 - val_accuracy: 0.4450\n",
      "Epoch 30/32\n",
      "79/79 [==============================] - 85s 1s/step - loss: 0.5929 - accuracy: 0.7784 - val_loss: 1.9094 - val_accuracy: 0.4510\n",
      "Epoch 31/32\n",
      "79/79 [==============================] - 74s 938ms/step - loss: 0.5745 - accuracy: 0.7899 - val_loss: 2.0686 - val_accuracy: 0.4480\n",
      "Epoch 32/32\n",
      "79/79 [==============================] - 77s 981ms/step - loss: 0.5536 - accuracy: 0.8031 - val_loss: 2.1912 - val_accuracy: 0.4430\n"
     ]
    }
   ],
   "source": [
    "model =  comFit(model,trainX, trainY,testX,textY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Facial Emotion Prediction\n",
    "\n",
    "capturing live frames:\n",
    "we used pc camera as source of video and we used openCV library.we used VideoCapture class with '0' as an argument for capturing videos from the default source of video(camera). We created an infinite loop to capture frames  from the vieo iteratively using the read method of VideoCapture class and convert the frame into grayscale.\n",
    "\n",
    "detecting facial region:\n",
    "We again make use of openCV's face detection api CascadeClassifier, to detect facial arias of the captured frame. The training data set we use for CascadeClassifier is haaarcascade_frontalface_default.xml. detectMultiScale returns the detected faces as list of rectangles and their locations.\n",
    "\n",
    "prediction:\n",
    "We loop through the detected face recangles and draw the rectangles on the screen. we select area of interst from the image based on the returned rectangle and the location of the image. convert the selected area into array of metrices containing pixles after resizing the image based on our training data. reduce the rgb representation by dividing it by 255 and give the image to our model for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-26255c9a2f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#saving our model for later prediction use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodelJson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelJson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#saving our model for later prediction use\n",
    "modelJson = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(modelJson)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our model for prediction\n",
    "model = model_from_json(open(\"model.json\", \"r\").read())\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "realPred(\"Basic Emotions Test.mp4\",model,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
