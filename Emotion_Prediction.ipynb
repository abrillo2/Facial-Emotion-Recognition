{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The Face data we used for training is taken from kaggle in Representation Learning: Facial Expression Recognition Challenge from 2013. the data consists of 48x48 pixel grayscale images of faces arranged in a row and seven categories/labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) of emotions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "#compie and fit the CNN model\n",
    "def comFit(model,trainX, trainY, testX=None, testY=None):\n",
    "    #Compliling the model\n",
    "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    #Training the model\n",
    "    model.fit(trainX, trainY,batch_size=64,epochs=32,verbose=1,shuffle=True)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,emotions):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "\n",
    "        if not val:\n",
    "            continue;\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.32, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=8)\n",
    "\n",
    "            predImg=testImg[y:y+w,x:x+h]\n",
    "            predImg=cv2.resize(predImg,(48,48))\n",
    "            predPixels = image.img_to_array(predImg)\n",
    "            predPixels = np.expand_dims(predPixels, axis = 0)\n",
    "            predPixels /= 255\n",
    "\n",
    "            pridectedList = model.predict(predPixels)\n",
    "\n",
    "            maxVal = np.argmax(pridectedList[0])\n",
    "            predEmotion = emotions[maxVal]\n",
    "            \n",
    "            path = os.path.join(\"emot\",predEmotion+\".png\")\n",
    "          \n",
    "            \n",
    "            emoji = cv2.imread(path)\n",
    "            emoji = cv2.resize(emoji,(100,100))\n",
    "            \n",
    "\n",
    "            img[int(y):int(y)+emoji.shape[0], int(x):int(x)+emoji.shape[1]] = emoji\n",
    "            cv2.putText(img, predEmotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(img, (1000, 700))\n",
    "        cv2.imshow('Analayzing Facial Emotion',resized_img)\n",
    "\n",
    "        if cv2.waitKey(10) == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "#CNN Model from training set\n",
    "def craeateModel(trainX):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(trainX.shape[1:])))\n",
    "    model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible target emotions\n",
    "emotions = ('happy', 'fear', 'disgust', 'angry', 'sad', 'surprise', 'neutral')\n",
    "#load the face data using panda from csv and extract triaining data from it.\n",
    "faceData = pd.read_csv('fer2013.csv').iterrows()\n",
    "\n",
    "trainingSize = 500\n",
    "trainY = []\n",
    "trainX = []\n",
    "\n",
    "for i,j in faceData:\n",
    "    \n",
    "    if(trainingSize == 0):\n",
    "        break;\n",
    "    else:\n",
    "        trainX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "        trainY.append(j['emotion'])\n",
    "    \n",
    "    trainingSize-=1\n",
    "\n",
    "#flatten the data array and normalize between 1 and 0 using mean and standard deviation for x\n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "trainY = np_utils.to_categorical(trainY, num_classes=7)\n",
    "\n",
    "trainX -= np.mean(trainX, axis=0)\n",
    "trainX /= np.std(trainX, axis=0)\n",
    "#reshape the matrices to represent each face data\n",
    "trainX = trainX.reshape(trainX.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature filtering\n",
    "We used a three layer Convolutional neural network for extracting features and we aplied maxpooling to reduce the dimensions of the data. Finally we flatten the image matrix and pass it through a fully connected layer to classify the images. \n",
    "\n",
    "since we only have one input(image pixels) and one output(the pridicted emotion, 0 to 6 ),We used sequential class from keras for creating the CNN model. We add each layer instance with conv2d method, which  creates convolution kernel(4,4) that is convolved with the layer input to produce a tensor of outputs.The activatioin function used is relu, which activiate each neuron(not all at once) to produce output for the next layer. The drop out rate used is 0.2.\n",
    "\n",
    "MaxPolling2D is used with window size and stride of 3 in each dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = craeateModel(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compile and train the model\n",
    "\n",
    "in the model configuration, with use adam optimizer and 'accuracy' as a metric to evaluate the model during the training and testing. And a loss/objective function used is crossentropy.\n",
    "\n",
    "in the triaining of the model, number of samples per gradient update is set to be 64, max iteration is set to be 32, verbosity mode used is progress bar during training and shuffle is on , to shuffle the training data before each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model =  comFit(model,trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Facial Emotion Prediction\n",
    "\n",
    "capturing live frames:\n",
    "we used pc camera as source of video and we used openCV library.we used VideoCapture class with '0' as an argument for capturing videos from the default source of video(camera). We created an infinite loop to capture frames  from the vieo iteratively using the read method of VideoCapture class and convert the frame into grayscale.\n",
    "\n",
    "detecting facial region:\n",
    "We again make use of openCV's face detection api CascadeClassifier, to detect facial arias of the captured frame. The training data set we use for CascadeClassifier is haaarcascade_frontalface_default.xml. detectMultiScale returns the detected faces as list of rectangles and their locations.\n",
    "\n",
    "prediction:\n",
    "We loop through the detected face recangles and draw the rectangles on the screen. we select area of interst from the image based on the returned rectangle and the location of the image. convert the selected area into array of metrices containing pixles after resizing the image based on our training data. reduce the rgb representation by dividing it by 255 and give the image to our model for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100,100,3) into shape (100,90,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ab38215bff39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrealPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How I Met Your Mother S04E13 Three Days of Snow (1080p x265 Joy).m4v\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6dfb9f6a3f28>\u001b[0m in \u001b[0;36mrealPred\u001b[0;34m(source, model, emotions)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredEmotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100,100,3) into shape (100,90,3)"
     ]
    }
   ],
   "source": [
    "realPred(\"How I Met Your Mother S04E13 Three Days of Snow (1080p x265 Joy).m4v\",model,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
